{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Hello, world!\n"
                }
            ], 
            "source": "println(\"Hello, world!\")"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val x: Int = 10\nval y: Int = 20\nx + y"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def fun() {\n    println(\"This is a function\")\n}\nfun()"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def getInfoFromBackend() : List[Any] = {\n    val dataList = List(1,\"Literature\",2,\"Science\")\n    dataList\n}\nprintln(getInfoFromBackend()(1))"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "case class Payment(label: String, year: Int, obols: Int)\nval record = Payment(\"Pharbelios\", 1, 100)"
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 39, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "100"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "record.obols"
        }, 
        {
            "source": "When you want to create a RDD from existing storage in driver program (which we would like to be parallelized). For example, converting an array to RDD, which is already created in a driver program.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val data = 1 to 10\nval distData = sc.parallelize(data)"
        }, 
        {
            "source": "In the above program, I first created an array for 10 elements and then I created a distributed data called RDD from that array using \u201cparallelize\u201d method. SparkContext has a parallelize method, which is used for creating the Spark RDD from an iterable already present in driver program.\n\nTo see the content of any RDD we can use \u201ccollect\u201d method. Let\u2019s see the content of distData.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "distData.collect()"
        }, 
        {
            "source": "**Now let\u2019s see all lines in it.**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 100, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "execution_count": 100, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array([I love solving data mining problems.], [I don't like solving data mining problems.], [I love solving data science problems.], [I don't like solving data science problems.])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "// The code was removed by DSX for sharing."
        }, 
        {
            "source": "A map transformation is useful when we need to transform a RDD by applying a function to each element. So how can we use map transformation on \u2018rdd\u2019 in our case?\nLet\u2019s calculate the length (number of characters) of each line in \u201ctext.txt\u201d", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 132, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 132, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array(36, 42, 37, 43)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val Lenght1 = lines.map(s => s(0).toString.length)\nLenght1.collect()"
        }, 
        {
            "execution_count": 139, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "execution_count": 139, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array((solving,4), (I,4), (problems.,4), (data,4))"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val palabras = lines.flatMap( s => s(0).toString.split(\" \")).map( w => (w,1)).reduceByKey(_ + _)\npalabras.take(100)\npalabras.filter(_._2.toInt > 2).take(199)"
        }, 
        {
            "execution_count": 138, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 138, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Array((solving,4), (I,4), (problems.,4), (data,4))"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "palabras.filter(s => s._2 > 2).take(100)"
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------+\n|                 _c0|\n+--------------------+\n|I love solving da...|\n|I don't like solv...|\n|I love solving da...|\n|I don't like solv...|\n+--------------------+\n\n"
                }
            ], 
            "source": "val dfData1 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"false\").\n    option(\"inferSchema\", \"true\").\n    load(cos.url(\"testbef2efc72e4f470d9a548eff336bbc99\", \"text.csv\"))\ndfData1.show(5)\n"
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "// Create the case classes for our domain\ncase class Department(id: String, name: String)\ncase class Employee(firstName: String, lastName: String, email: String, salary: Int)\ncase class DepartmentWithEmployees(department: Department, employees: Seq[Employee])\n\n// Create the Departments\nval department1 = new Department(\"123456\", \"Computer Science\")\nval department2 = new Department(\"789012\", \"Mechanical Engineering\")\nval department3 = new Department(\"345678\", \"Theater and Drama\")\nval department4 = new Department(\"901234\", \"Indoor Recreation\")\n\n// Create the Employees\nval employee1 = new Employee(\"michael\", \"armbrust\", \"no-reply@berkeley.edu\", 100000)\nval employee2 = new Employee(\"xiangrui\", \"meng\", \"no-reply@stanford.edu\", 120000)\nval employee3 = new Employee(\"matei\", null, \"no-reply@waterloo.edu\", 140000)\nval employee4 = new Employee(null, \"wendell\", \"no-reply@princeton.edu\", 160000)\n\n// Create the DepartmentWithEmployees instances from Departments and Employees\nval departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))\nval departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))\nval departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee1, employee4))\nval departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))\n"
        }, 
        {
            "execution_count": 83, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------------------+-----------------------------------------------------------------------------------------------+\n|department                     |employees                                                                                      |\n+-------------------------------+-----------------------------------------------------------------------------------------------+\n|[123456,Computer Science]      |[[michael,armbrust,no-reply@berkeley.edu,100000], [xiangrui,meng,no-reply@stanford.edu,120000]]|\n|[789012,Mechanical Engineering]|[[matei,null,no-reply@waterloo.edu,140000], [null,wendell,no-reply@princeton.edu,160000]]      |\n+-------------------------------+-----------------------------------------------------------------------------------------------+\n\n"
                }
            ], 
            "source": "// Create the first DataFrame from a List of the Case Classes.\nval departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)\nval df1 = departmentsWithEmployeesSeq1.toDF()\ndf1.show(false)"
        }, 
        {
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------------+-----------------------------------------------------------------------------------------------+\n|department                |employees                                                                                      |\n+--------------------------+-----------------------------------------------------------------------------------------------+\n|[345678,Theater and Drama]|[[michael,armbrust,no-reply@berkeley.edu,100000], [null,wendell,no-reply@princeton.edu,160000]]|\n|[901234,Indoor Recreation]|[[xiangrui,meng,no-reply@stanford.edu,120000], [matei,null,no-reply@waterloo.edu,140000]]      |\n+--------------------------+-----------------------------------------------------------------------------------------------+\n\n"
                }
            ], 
            "source": "// Create a 2nd DataFrame from a List of Case Classes.\nval departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)\nval df2 = departmentsWithEmployeesSeq2.toDF()\ndf2.show(false)"
        }, 
        {
            "source": "**WORKING WITH DATAFRAMES**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+----+\n|NUM|WORD|\n+---+----+\n|101|   a|\n|102|   b|\n+---+----+\n\n"
                }
            ], 
            "source": "// Columns operation\nval p = Seq((1,\"a\"),(2,\"b\")).toDF(\"NUM\",\"WORD\")\np.withColumn(\"NUM\",$\"NUM\"+100).show"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Stage 188:============================>                            (1 + 1) / 2]+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|\n|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|                14|              null|    1057|\n|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "\nval dfData2 = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(cos.url(\"testbef2efc72e4f470d9a548eff336bbc99\", \"train.csv\"))\ndfData2.show(5)\n"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "550068                                                                          \nroot\n |-- User_ID: integer (nullable = true)\n |-- Product_ID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- Occupation: integer (nullable = true)\n |-- City_Category: string (nullable = true)\n |-- Stay_In_Current_City_Years: string (nullable = true)\n |-- Marital_Status: integer (nullable = true)\n |-- Product_Category_1: integer (nullable = true)\n |-- Product_Category_2: integer (nullable = true)\n |-- Product_Category_3: integer (nullable = true)\n |-- Purchase: integer (nullable = true)\n\n"
                }
            ], 
            "source": "println(dfData2.count)\ndfData2.printSchema()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+\n|  Age|\n+-----+\n| 0-17|\n| 0-17|\n| 0-17|\n| 0-17|\n|  55+|\n|26-35|\n|46-50|\n|46-50|\n|46-50|\n|26-35|\n+-----+\nonly showing top 10 rows\n\n"
                }
            ], 
            "source": "dfData2.select(\"Age\").show(10)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------+\n|Purchase|\n+--------+\n|   15200|\n|   15227|\n|   19215|\n|   15854|\n|   15686|\n|   15665|\n|   13055|\n|   11788|\n|   19614|\n|   11927|\n+--------+\nonly showing top 10 rows\n\n+--------+\n|Purchase|\n+--------+\n|   15200|\n|   15227|\n|   19215|\n|   15854|\n|   15686|\n|   15665|\n|   13055|\n|   11788|\n|   19614|\n|   11927|\n+--------+\nonly showing top 10 rows\n\n+--------+\n|Purchase|\n+--------+\n|   15200|\n|   15227|\n|   19215|\n|   15854|\n|   15686|\n|   15665|\n|   13055|\n|   11788|\n|   19614|\n|   11927|\n+--------+\nonly showing top 10 rows\n\n"
                }
            ], 
            "source": "import org.apache.spark.sql.functions.col\n\ndfData2.filter(dfData2(\"Purchase\") >= 10000).select(\"Purchase\").show(10)\ndfData2.filter($\"Purchase\" >= 10000).select($\"Purchase\").show(10)\ndfData2.filter(col(\"Purchase\") >= 10000).select(col(\"Purchase\")).show(10)"
        }, 
        {
            "source": "### Apply SQL queries on DataFrame\nTo apply queries on DataFrame You need to register DataFrame(df) as table. Let\u2019s first register df as temporary table called (B_friday).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----+\n| Age|\n+----+\n|0-17|\n|0-17|\n|0-17|\n|0-17|\n| 55+|\n+----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val sqlContext = spark.sqlContext\n\ndfData2.registerTempTable(\"B_friday\")\nsqlContext.sql(\"select Age from B_friday\").show(5)"
        }, 
        {
            "source": "### Building a machine learning model\nIf you have come this far, you are in for a treat! I\u2019ll complete this tutorial by building a machine learning model.\n\nI will use only three dependent features and the independent variable in df1. Let\u2019s create a DataFrame df1 which has only 4 columns (3 dependent and 1 target).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val df1 = dfData2.select(\"User_ID\",\"Occupation\",\"Marital_Status\",\"Purchase\")"
        }, 
        {
            "source": "In above DataFrame df1 \u201cUser_ID\u201d,\u201dOccupation\u201d and \u201cMarital_Status\u201d are features and \u201cPurchase\u201d is target column.\n\nLet\u2019s try to create a formula for Machine learning model like we do in R. First, we need to import RFormula. Then we need to specify the dependent and independent column inside this formula. We also have to specify the names for features column and label column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import org.apache.spark.ml.feature.RFormula\nval formula = new RFormula().setFormula(\"Purchase ~ User_ID+Occupation+Marital_Status\").setFeaturesCol(\"features\").setLabelCol(\"label\")"
        }, 
        {
            "source": "After creating the formula, we need to fit this formula on df1 and transform df1 through this formula. Let\u2019s fit this formula.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "class org.apache.spark.sql.Dataset\n"
                }
            ], 
            "source": "val train = formula.fit(df1).transform(df1)\nprintln(train.getClass)"
        }, 
        {
            "source": "### Applying Linear Regression on train\nAfter applying the RFormula and transforming the DataFrame, we now need to develop the machine learning model on this data. I want to apply a Linear Regression for this task. Let us import a Linear regression and apply on train. Before fitting the model, I am setting the hyperparameters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Stage 219:============================>                            (1 + 1) / 2]"
                }
            ], 
            "source": "import org.apache.spark.ml.regression.LinearRegression\nval lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\nval lrModel = lr.fit(train)"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Coefficients: [0.015092117950462813,16.121177918472583,-10.520581613126115] Intercept: -5999.757125186284\n"
                }
            ], 
            "source": "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}